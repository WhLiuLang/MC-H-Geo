# -*- coding: utf-8 -*-
"""
æœ€ç»ˆçš„å±‚æ¬¡åŒ–æ¨¡å‹è®­ç»ƒè„šæœ¬ (ä½¿ç”¨æœ€ä¼˜ç‰¹å¾é›†)

åŠŸèƒ½:
1.  åˆ†åˆ«åŠ è½½ä¸ºâ€œå®ˆé—¨å‘˜â€å’Œâ€œå²©çŸ³ä¸“å®¶â€ç­›é€‰å‡ºçš„æœ€ä¼˜ç‰¹å¾é›†ã€‚
2.  ä¸å†è¿›è¡Œä»»ä½•åŠ¨æ€ç‰¹å¾å·¥ç¨‹ï¼Œç›´æ¥ä½¿ç”¨ç­›é€‰å¥½çš„ç‰¹å¾ã€‚
3.  è®­ç»ƒæœ€ç»ˆçš„ã€é«˜åº¦ä¼˜åŒ–çš„å±‚æ¬¡åŒ–æ¨¡å‹ï¼Œå¹¶è¿›è¡Œè¯„ä¼°ã€‚
"""
import os
import time
import csv
import warnings
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_sample_weight
from xgboost import XGBClassifier
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet

warnings.filterwarnings("ignore")

# ======================= 1. ç”¨æˆ·é…ç½®åŒº =======================
# --- è¾“å…¥æ–‡ä»¶è·¯å¾„ (å…³é”®ä¿®æ”¹) ---
# æŒ‡å‘æ‚¨ç­›é€‰å‡ºçš„ä¸¤ä¸ªæœ€ä¼˜ç‰¹å¾é›†
GOALKEEPER_DATA_CSV = "D:/ç ”ç©¶ç”Ÿ/æ¯•ä¸šè®ºæ–‡æ•°æ®/å‰ç‹å®¶æ²³3å·éœ²å¤´/å²©æ€§æ ‡å‡†æ ·/æ ‡å‡†ä½“ç´ æ ·æœ¬/goalkeeper_optimal_features_1.csv"
ROCKEXPERT_DATA_CSV = "D:/ç ”ç©¶ç”Ÿ/æ¯•ä¸šè®ºæ–‡æ•°æ®/å‰ç‹å®¶æ²³3å·éœ²å¤´/å²©æ€§æ ‡å‡†æ ·/æ ‡å‡†ä½“ç´ æ ·æœ¬/rockexpert_optimal_features_1.csv"

# --- è¾“å‡ºä¸å‚æ•°é…ç½® ---
OUTPUT_DIR = "final_model_results"
OUTPUT_PDF = os.path.join(OUTPUT_DIR, "final_model_summary_modified.pdf")
RANDOM_STATE = 42
LABELS = ["mudstone", "sandstone", "siltstone", "vegetation"]
N_CLASSES = len(LABELS)
VEGETATION_LABEL = 3
BASE_LIST = ["RF", "XGB", "MLP"]
STACKING_FOLDS = 5


# ======================= 2. æ¨¡å‹ä¸è¾…åŠ©å‡½æ•° (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´) =======================
def ensure_1d(y): return np.asarray(y).ravel()


def build_branches(df):
    feat_cols = [c for c in df.columns if c not in ['label', 'filename']]
    numeric_cols = [c for c in feat_cols if np.issubdtype(df[c].dtype, np.number)]
    patterns = {'geom': [r'roughness', r'linearity', r'planarity', r'sphericity', r'point_den', r'geom_pca'],
                'spectral': [r'ref_', r'refnorm_', r'amp_'], 'texture': [r'glcm_', r'fft_', r'tex_feat']}
    branches = {}
    for bname, pat_list in patterns.items():
        branch_cols = [c for c in numeric_cols if any(re.search(p, c) for p in pat_list)]
        if branch_cols: branches[bname] = branch_cols
    if not branches: branches['full_features'] = numeric_cols
    return branches


def get_base_model(name):
    if name == "RF": return RandomForestClassifier(n_estimators=300, max_depth=16, min_samples_leaf=1,
                                                   max_features="sqrt", class_weight="balanced_subsample",
                                                   random_state=RANDOM_STATE, n_jobs=-1)
    if name == "XGB": return XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.05, subsample=0.9,
                                           colsample_bytree=0.9, reg_lambda=6.0, reg_alpha=0.5,
                                           random_state=RANDOM_STATE, use_label_encoder=False, eval_metric="mlogloss",
                                           n_jobs=-1, tree_method="hist")
    if name == "MLP": return MLPClassifier(hidden_layer_sizes=(128, 64), alpha=3e-4, max_iter=600,
                                           random_state=RANDOM_STATE)
    raise ValueError(name)


def plot_cm(cm, title, out_png, labels_list):
    plt.figure(figsize=(6, 5));
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels_list, yticklabels=labels_list)
    plt.xlabel("Predicted");
    plt.ylabel("Actual");
    plt.title(title);
    plt.tight_layout();
    plt.savefig(out_png, dpi=150);
    plt.close()


# ======================= 3. æ ¸å¿ƒè®­ç»ƒä¸é¢„æµ‹å‡½æ•° (æœ€ç»ˆç‰ˆ) =======================
def train_goalkeeper(X_df, y):
    print("\n===== STAGE 1: Training Goalkeeper (on Optimal Features) =====")
    y_binary = np.where(y == VEGETATION_LABEL, 1, 0)
    print(f"Goalkeeper training data: X shape={X_df.shape}, y_binary distribution={Counter(y_binary)}")
    model = RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_leaf=3, class_weight='balanced',
                                   random_state=RANDOM_STATE, n_jobs=-1)
    model.fit(X_df.values, y_binary)
    print("âœ… Goalkeeper (Random Forest) trained.")
    return model


def train_rock_expert(X_expert_df, y):
    print("\n===== STAGE 2: Training Rock Expert (on Optimal Features) =====")
    rock_indices = np.where(y != VEGETATION_LABEL)[0]
    X_rock = X_expert_df.iloc[rock_indices].reset_index(drop=True)
    y_rock = y[rock_indices]
    le = LabelEncoder();
    y_rock_enc = le.fit_transform(y_rock)
    print(f"Rock Expert training data: X shape={X_rock.shape}, y_rock distribution={Counter(y_rock)}")
    branches = build_branches(X_rock)
    for bname, cols in branches.items(): print(f"  - Branch '{bname}': {len(cols)} features")
    min_class_count = min(np.bincount(y_rock_enc));
    n_splits = min(STACKING_FOLDS, min_class_count)
    if n_splits < 2: print("âš ï¸ Not enough samples for stacking."); return None
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    oof_meta_features = []
    for bname, b_cols in branches.items():
        X_branch = X_rock[b_cols];
        scaler = StandardScaler().fit(X_branch);
        X_branch_scaled = scaler.transform(X_branch)
        for mname in BASE_LIST:
            oof_preds_model = np.zeros((len(X_rock), len(le.classes_)))
            for fold, (tr_idx, te_idx) in enumerate(skf.split(X_branch_scaled, y_rock_enc)):
                X_tr, X_te = X_branch_scaled[tr_idx], X_branch_scaled[te_idx];
                y_tr = y_rock_enc[tr_idx]
                clf = get_base_model(mname);
                sw = compute_sample_weight(class_weight='balanced', y=y_tr)
                try:
                    clf.fit(X_tr, y_tr, sample_weight=sw)
                except TypeError:
                    clf.fit(X_tr, y_tr)
                oof_preds_model[te_idx] = clf.predict_proba(X_te)
            oof_meta_features.append(oof_preds_model)
    meta_features = np.hstack(oof_meta_features)
    meta_learner = LogisticRegression(max_iter=1000, multi_class="multinomial", class_weight="balanced",
                                      random_state=RANDOM_STATE)
    meta_learner.fit(meta_features, y_rock_enc)
    final_base_models = {}
    for bname, b_cols in branches.items():
        X_branch = X_rock[b_cols];
        scaler = StandardScaler().fit(X_branch);
        X_branch_scaled = scaler.transform(X_branch)
        final_base_models[bname] = {'scaler': scaler, 'models': {}}
        for mname in BASE_LIST:
            clf = get_base_model(mname);
            sw = compute_sample_weight(class_weight='balanced', y=y_rock_enc)
            try:
                clf.fit(X_branch_scaled, y_rock_enc, sample_weight=sw)
            except TypeError:
                clf.fit(X_branch_scaled, y_rock_enc)
            final_base_models[bname]['models'][mname] = clf
    print("âœ… Rock Expert trained.")
    return {"meta_learner": meta_learner, "base_models": final_base_models, "branches": branches, "label_encoder": le}


def predict_hierarchical(X_goalkeeper_df, X_expert_df, goalkeeper, rock_expert_bundle):
    goalkeeper_preds = goalkeeper.predict(X_goalkeeper_df.values)
    final_preds = np.full(len(X_goalkeeper_df), -1, dtype=int)
    veg_indices = np.where(goalkeeper_preds == 1)[0];
    final_preds[veg_indices] = VEGETATION_LABEL
    rock_indices = np.where(goalkeeper_preds == 0)[0]
    if len(rock_indices) > 0 and rock_expert_bundle is not None:
        X_rock_to_predict = X_expert_df.iloc[rock_indices]
        meta_features_to_predict = []
        base_models_bundle, branches = rock_expert_bundle['base_models'], rock_expert_bundle['branches']
        for bname, b_cols in branches.items():
            scaler = base_models_bundle[bname]['scaler']
            X_branch_scaled = scaler.transform(X_rock_to_predict[b_cols])
            for mname in BASE_LIST:
                clf = base_models_bundle[bname]['models'][mname]
                meta_features_to_predict.append(clf.predict_proba(X_branch_scaled))
        meta_features_test = np.hstack(meta_features_to_predict)
        meta_learner, le = rock_expert_bundle['meta_learner'], rock_expert_bundle['label_encoder']
        expert_preds_enc = meta_learner.predict(meta_features_test)
        final_preds[rock_indices] = le.inverse_transform(expert_preds_enc)
    return final_preds


# ======================= 4. ä¸»æ‰§è¡Œæµç¨‹ (æœ€ç»ˆç‰ˆ) =======================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("ğŸ“¥ Loading Final Optimal Datasets...")
    # --- åŠ è½½ä¸¤ä¸ªç‹¬ç«‹çš„ã€æœ€ä¼˜åŒ–çš„æ•°æ®é›† ---
    try:
        # <<< --- æ ¸å¿ƒä¿®æ”¹ï¼šä¸º read_csv æ·»åŠ  encoding='gbk' å‚æ•° --- >>>
        df_goalkeeper = pd.read_csv(GOALKEEPER_DATA_CSV, encoding='gbk')
        df_rockexpert = pd.read_csv(ROCKEXPERT_DATA_CSV, encoding='gbk')
        # <<< --- ä¿®æ”¹ç»“æŸ --- >>>

        print("è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸã€‚")
    except FileNotFoundError as e:
        print(f"åŠ è½½è®­ç»ƒé›†æ—¶å‡ºé”™: {e}");
        return
    except UnicodeDecodeError:
        print(f"ä½¿ç”¨'gbk'ç¼–ç è¯»å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨'utf-8'...")
        try:
            # å¦‚æœgbkå¤±è´¥ï¼Œä¹Ÿå°è¯•ä¸€ä¸‹utf-8ï¼Œå¢åŠ ç¨³å¥æ€§
            df_goalkeeper = pd.read_csv(GOALKEEPER_DATA_CSV, encoding='utf-8')
            df_rockexpert = pd.read_csv(ROCKEXPERT_DATA_CSV, encoding='utf-8')
        except Exception as e:
            print(f"å°è¯•å¤šç§ç¼–ç åï¼Œè¯»å–è®­ç»ƒé›†ä»ç„¶å¤±è´¥: {e}");
            return

    # ä»ä»»ä¸€æ•°æ®é›†ä¸­æå–æ ‡ç­¾å’Œç´¢å¼•ï¼ˆå®ƒä»¬åº”è¯¥æ˜¯ä¸€è‡´çš„ï¼‰
    y_all = df_goalkeeper['label'].values
    y_all = np.where(y_all == 4, 0, y_all)  # ç¡®ä¿æ ‡ç­¾å·²åˆå¹¶

    X_goalkeeper_df = df_goalkeeper.drop(columns=['label', 'filename'], errors='ignore')
    X_rockexpert_df = df_rockexpert.drop(columns=['label', 'filename'], errors='ignore')

    # å¯¹é½æ£€æŸ¥
    if len(X_goalkeeper_df) != len(X_rockexpert_df):
        print("Error: The two optimal datasets have different numbers of rows!");
        return

    print(f"Goalkeeper feature set shape: {X_goalkeeper_df.shape}")
    print(f"Rock Expert feature set shape: {X_rockexpert_df.shape}")
    print(f"Label distribution: {Counter(y_all)}")

    # åˆ›å»ºä¸€è‡´çš„è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ç´¢å¼•
    indices = np.arange(len(y_all))
    tr_idx, val_idx, y_tr, y_val = train_test_split(indices, y_all, test_size=0.2, random_state=RANDOM_STATE,
                                                    stratify=y_all)

    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ’åˆ†å¯¹åº”çš„æ•°æ®
    X_tr_goalkeeper, X_val_goalkeeper = X_goalkeeper_df.iloc[tr_idx], X_goalkeeper_df.iloc[val_idx]
    X_tr_rockexpert, X_val_rockexpert = X_rockexpert_df.iloc[tr_idx], X_rockexpert_df.iloc[val_idx]

    print(f"\nTraining set size: {len(y_tr)}, Validation set size: {len(y_val)}")

    # ---------- Train Hierarchical Model ----------
    t_train_start = time.time()

    goalkeeper_model = train_goalkeeper(X_tr_goalkeeper, y_tr)
    rock_expert_bundle = train_rock_expert(X_tr_rockexpert, y_tr)

    train_dt = time.time() - t_train_start
    print(f"\nTotal training time: {train_dt:.2f} seconds")

    # ---------- Evaluate on Validation Set ----------
    print("\nğŸ”¬ Evaluating on validation set...")
    y_pred_val = predict_hierarchical(X_val_goalkeeper, X_val_rockexpert, goalkeeper_model, rock_expert_bundle)

    val_acc = accuracy_score(y_val, y_pred_val)
    val_f1m = f1_score(y_val, y_pred_val, average='macro')

    print(f"\n>>> FINAL Validation Accuracy: {val_acc:.4f}")
    print(f">>> FINAL Validation Macro-F1: {val_f1m:.4f}")

    # ---------- Reporting ----------
    cm_val = confusion_matrix(y_val, y_pred_val, labels=np.arange(N_CLASSES))
    cm_png = os.path.join(OUTPUT_DIR, "Final_Model_Validation_CM.png")
    plot_cm(cm_val, f"Final Hierarchical Model (Validation Acc: {val_acc:.3f})", cm_png, LABELS)

    # ... CSV and PDF reporting code (ä¸ä¹‹å‰ç‰ˆæœ¬ä¸€è‡´) ...
    csv_file = os.path.join(OUTPUT_DIR, "final_model_results.csv")
    with open(csv_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f);
        writer.writerow(["Model", "Validation-Accuracy", "Validation-Macro-F1", "Training-Time(s)"])
        writer.writerow(["Hierarchical_Optimized", f"{val_acc:.4f}", f"{val_f1m:.4f}", f"{train_dt:.2f}"])
    print(f"\nCSV report saved to {csv_file}")

    styles = getSampleStyleSheet();
    doc = SimpleDocTemplate(OUTPUT_PDF, pagesize=A4)
    story = [Paragraph("Final Optimized Model Report", styles["Title"]), Spacer(1, 12),
             Paragraph(f"Validation Accuracy: {val_acc:.4f}", styles["h2"]),
             Paragraph(f"Validation Macro-F1 Score: {val_f1m:.4f}", styles["h2"]), Spacer(1, 12),
             Paragraph("Validation Confusion Matrix:", styles["h3"]), Image(cm_png, width=400, height=320)]
    doc.build(story);
    print(f"PDF report saved to {OUTPUT_PDF}")


if __name__ == "__main__":
    main()